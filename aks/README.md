__This is not an Elastic official documentation, just my notes and observations during the deployment of a small ECK cluster on AKS.__

## 0. Prerequisites

To run this demo, you need an existing Azure account and an existing resource group.

## 1. Terraform

### 1.1. Initialize terraform

Initialize terraform:
```
cd terraform
```
```
terraform init
```

### 1.2. Set variables

Before invoking terraform, make sure to change the defaults in the [variables](terraform/variables.tf.sample) file to your own values. For that, take the sample file and modify the defaults:
```
cp variables.tf.sample variables.tf
```
```
vi variables.tf
```
_NOTE: Alternatively, you can use a variables file as parameter when invoking terraform, instead of using defaults._

### 1.3 Deploy

Deploy Azure resources with terraform:
```
terraform apply
```
Following Azure resources are deployed, according with [main.tf](terraform/main.tf):

 * AKS K8s Cluster
 * Public IP address
 * The ingress-nginx controller

To create the public IP address, we need to use the auto-generated resource group of the AKS cluster node-pool, and NOT the resource group of the AKS cluster itself. Otherwise, the ingress controller won't be able to assign the IP to its LoadBalancer service. 

The ingress-nginx controller is installed via helm, using terraform. It will be deployed in the same cluster we just created. For that, uses the kubeconfig generated during the creation of the AKS cluster. Upon installation, the ingress-nginx controller creates a service of type LoadBalancer and assignes the public IP we created as external IP.

For the IP address, we define what Azure calls a DNS label. With this, it's possible to associate the IP with an by Azure auto-generated hostname, passing the corresponding annotation to the ingress controller upon creation. With this hostname, we'll be able later to access kibana. It's also possilbe to define a DNS CNAME record to associate a custom hostname with the host, but for demo purposes I'm just using the auto-generated hostname.

As output, we have defined following items:

 * a kubeconfig.json file is genereated and stored in the current directory
 * the resource group name
 * the kubernetes cluster name

## 2. Kustomize

Now we have our AKS cluster, the ingress-controller deployed on it with a public IP address, we'll proceed to install the K8s resources with Kustomize. 

### 2.1. Set kubeconfig

To access the cluster, we need to set kubeconfig correclty. An easy way to do it is use the azure command line tool, after the generation of the terraform resources is done:
```
az aks get-credentials --resource-group $(terraform output -raw resource_group_name) --name $(terraform output -raw kubernetes_cluster_name)
```
Now we can interact directly with our k8s cluster via kubectl.
```
cd ../k8s 
```

### 2.2 Modify the ingress

Before deploying with kubectl, we need to modify the hostname in the [ingress](k8s/ingress.yaml.sample) file, to match the one auto-generated by terraform. For the time being, I'm doing this manually, fetching the hostname from the Azure Console. Just copy the provided sample and modify it accordingly:l
```
cp ingress.yaml.sample ingress.yaml
```
```
vi ingress.yaml 
```

### 2.3 Deploy
Once the change to the ingress is done, deploy K8s resources:
```
kubectl apply -k .
```
Following K8s resources are deployed, according with [kustomization.yaml](k8s/kustomization.yaml):

 * The ECK Custom Resource Definitions, which allow us to manage the elastic resources as K8s resources
 * The ECK operator, which act as the controller for our elastic custom resources
 * The cert-manager, to generate a trustworthy TLS certificate for Kibana
 * The cluster issuer, to be able to use Let's Encrypt as the Certificate Authority 
 * 1 elasticsearch cluster with 1 node
 * 1 kibana server
 * The ingress, so we can expose the kibana service through a public URL
 * 1 fleet-agent server
 
_NOTE: I always need to run the kubectl command twice. For some reasons, when it comes to deploying the elastic resources, the CRDs are not ready yet. So I run a second time, and then it works. It must be a better way to control this, though._

After waiting a little bit, you should be able to access the kibana service under the auto-generated hostname mentioned before. 
